{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Map987/download-anime-mag-/blob/main/download%20original%20keyvisual.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AMLNjIxZ4Nwr",
        "outputId": "a11df032-6a81-4568-ae23-f03f018170e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'download-anime-mag-'...\n",
            "remote: Enumerating objects: 105, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 105 (delta 22), reused 0 (delta 0), pack-reused 61\u001b[K\n",
            "Receiving objects: 100% (105/105), 2.72 MiB | 11.70 MiB/s, done.\n",
            "Resolving deltas: 100% (77/77), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Map987/download-anime-mag-.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Y6FG_-J34boZ",
        "outputId": "c2340e30-1ccc-422f-df8a-755c51fa9ae3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "folders = [\"/content/sample_data/out\", \"/content/sample_data/outtxt\", \"/content/sample_data/imagebig/\"]\n",
        "for folder in folders:\n",
        "    os.makedirs(folder, exist_ok=True)"
      ],
      "metadata": {
        "id": "IhdnHQxv4YBW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import xml.etree.ElementTree as ET\n",
        "from typing import List\n",
        "\n",
        "def extract_links(xml_string: str) -> List[str]:\n",
        "    root = ET.fromstring(xml_string)\n",
        "    links = []\n",
        "    for url in root.findall('{http://www.sitemaps.org/schemas/sitemap/0.9}url'):\n",
        "        image_loc = url.find('{http://www.google.com/schemas/sitemap-image/1.1}image').find('{http://www.google.com/schemas/sitemap-image/1.1}loc').text\n",
        "        if image_loc:\n",
        "            links.append(image_loc)\n",
        "    return links"
      ],
      "metadata": {
        "id": "MCNkHReC4orR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_links_to_file(file_path: str, links: List[str]) -> None:\n",
        "    with open(file_path, 'w') as f:\n",
        "        for link in links:\n",
        "            f.write(link)"
      ],
      "metadata": {
        "id": "_9N6QNac4rxN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_folder = '/content/download-anime-mag-/'\n",
        "output_folder = '/content/sample_data/outputtxt/'\n",
        "\n",
        "for file_name in os.listdir(input_folder):\n",
        "    if file_name.endswith('.xml'):\n",
        "        input_path = os.path.join(input_folder, file_name)\n",
        "        with open(input_path, 'r') as f:\n",
        "            xml_string = f.read()\n",
        "        links = extract_links(xml_string)\n",
        "        output_path = os.path.join(output_folder, file_name[:-4] + '.txt')\n",
        "        write_links_to_file(output_path, links)\n",
        "        print(f'{output_path} written successfully.\\n')"
      ],
      "metadata": {
        "id": "4vIBCHGO4voj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "input_folder_path = '/content/sample_data/outputtxt/'\n",
        "output_folder_path = '/content/sample_data/outtxt/'\n",
        "\n",
        "# get all .txt files in input folder\n",
        "files = [f for f in os.listdir(input_folder_path) if os.path.isfile(os.path.join(input_folder_path, f)) and f.endswith('.txt')]\n",
        "\n",
        "for file_name in files:\n",
        "    # open input file\n",
        "    input_file_path = os.path.join(input_folder_path, file_name)\n",
        "    with open(input_file_path, 'r') as input_file:\n",
        "        file_content = input_file.read()\n",
        "\n",
        "    # add newline before each https\n",
        "    file_content = file_content.replace('https://', 'https://')\n",
        "\n",
        "    # create output file\n",
        "    output_file_path = os.path.join(output_folder_path, file_name)\n",
        "    with open(output_file_path, 'w') as output_file:\n",
        "        output_file.write(file_content)"
      ],
      "metadata": {
        "id": "PduZW4YQ4zUq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "outtxt_dir = \"/content/sample_data/outtxt\"\n",
        "data_dir = \"/content/sample_data/\"\n",
        "\n",
        "# 遍历outtxt_dir目录中的所有txt文件\n",
        "for filename in os.listdir(outtxt_dir):\n",
        "    if filename.endswith(\".txt\") and filename.startswith(\"image-sitemap-\"):\n",
        "        # 获取X值\n",
        "        x = filename.split(\"-\")[-1].split(\".\")[0]\n",
        "\n",
        "        # 创建imageX文件夹\n",
        "        folder = os.path.join(data_dir, f\"image{x}\")\n",
        "        os.makedirs(folder, exist_ok=True)"
      ],
      "metadata": {
        "id": "66PtkiDb40ev"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h_l5TT6SYg6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "想要下载全部 执行下面这一个就可以了\n",
        "就可以了\n",
        "剩下的不用搞,，如果搞 带有\n",
        "-e1671875922994这些的\n",
        "下面不要执行，跳到末尾倒数第二个执行"
      ],
      "metadata": {
        "id": "imiHCMu5XwAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "def download(url, output_folder_path):\n",
        "    response = requests.get(url)\n",
        "    file_name = os.path.basename(url)\n",
        "    file_path = os.path.join(output_folder_path, file_name)\n",
        "\n",
        "    with open(file_path, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "        print(f\"File {file_name} downloaded successfully.\")\n",
        "\n",
        "def process_txt(x):\n",
        "    url_file_path = f\"/content/sample_data/outtxt/image-sitemap-{x}.txt\"\n",
        "    output_folder_path = f\"/content/sample_data/image{x}/\"\n",
        "    destination_folder = '/content/sample_data/imagebig'\n",
        "\n",
        "    try:\n",
        "        with open(url_file_path, 'r') as url_file:\n",
        "            file_content = url_file.read()\n",
        "\n",
        "            urls = file_content.split('https')\n",
        "            urls = ['https' + url for url in urls if url.endswith('.jpg') or url.endswith('.png')]\n",
        "\n",
        "            count = len(urls)\n",
        "            with ThreadPoolExecutor(max_workers=64) as executor:\n",
        "                futures = [executor.submit(download, url, output_folder_path) for url in urls]\n",
        "                for future in as_completed(futures):\n",
        "                    future.result()\n",
        "\n",
        "        print(f\"Downloaded {count} files for image{x}.\")\n",
        "\n",
        "        source_folder = output_folder_path\n",
        "        destination_folder = os.path.join(destination_folder, '')\n",
        "        zip_filename = f\"image{x}.zip\"\n",
        "        zip_filepath = os.path.join(destination_folder, zip_filename)\n",
        "\n",
        "        with zipfile.ZipFile(zip_filepath, 'w') as myzip:\n",
        "            for filename in os.listdir(source_folder):\n",
        "                file_path = os.path.join(source_folder, filename)\n",
        "                myzip.write(file_path, filename)\n",
        "\n",
        "        print(\"Files have been successfully zipped for image{x} to\", zip_filepath)\n",
        "\n",
        "        source_file = zip_filepath\n",
        "        destination_folder = '/content/drive/MyDrive'\n",
        "\n",
        "        shutil.copy(source_file, destination_folder)\n",
        "        print(f\"Zip file for image{x} has been successfully copied to Google Drive folder.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {url_file_path}: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    txt_files = list(range(6, 58)) + ['55_1', '25_1', '57_1']\n",
        "\n",
        "    for file in txt_files:\n",
        "        print(f\"Processing image{file}:\")\n",
        "        process_txt(file)"
      ],
      "metadata": {
        "id": "ej76ZF9PBtcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U2rD98-FXqfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "def download(url, output_folder_path):\n",
        "    response = requests.get(url)\n",
        "    file_name = os.path.basename(url)\n",
        "    file_path = os.path.join(output_folder_path, file_name)\n",
        "\n",
        "    with open(file_path, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "        print(f\"File {file_name} downloaded successfully.\")\n",
        "\n",
        "def process_txt(x):\n",
        "    if x in range(68, 70):\n",
        "        print(f\"Skipping image{x} as it is in exclusion list.\")\n",
        "        return\n",
        "\n",
        "    url_file_path = f\"/content/sample_data/out/image-sitemap-{x}.txt\"\n",
        "    output_folder_path = f\"/content/sample_data/image{x}/\"\n",
        "    destination_folder = '/content/sample_data/imagebig'\n",
        "\n",
        "    try:\n",
        "        with open(url_file_path, 'r') as url_file:\n",
        "            file_content = url_file.read()\n",
        "\n",
        "            urls = file_content.split('https')\n",
        "            urls = ['https' + url for url in urls if url.endswith('.jpg') or url.endswith('.png')]\n",
        "\n",
        "            count = len(urls)\n",
        "            with ThreadPoolExecutor(max_workers=64) as executor:\n",
        "                futures = [executor.submit(download, url, output_folder_path) for url in urls]\n",
        "                for future in as_completed(futures):\n",
        "                    future.result()\n",
        "\n",
        "        print(f\"Downloaded {count} files for image{x}.\")\n",
        "\n",
        "        source_folder = output_folder_path\n",
        "        destination_folder = os.path.join(destination_folder, '')\n",
        "        zip_filename = f\"image{x}.zip\"\n",
        "        zip_filepath = os.path.join(destination_folder, zip_filename)\n",
        "\n",
        "        with zipfile.ZipFile(zip_filepath, 'w') as myzip:\n",
        "            for filename in os.listdir(source_folder):\n",
        "                file_path = os.path.join(source_folder, filename)\n",
        "                myzip.write(file_path, filename)\n",
        "\n",
        "        print(\"Files have been successfully zipped for image{x} to\", zip_filepath)\n",
        "\n",
        "        source_file = zip_filepath\n",
        "        destination_folder = '/content/drive/MyDrive'\n",
        "\n",
        "        shutil.copy(source_file, destination_folder)\n",
        "        print(f\"Zip file for image{x} has been successfully copied to Google Drive folder.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {url_file_path}: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    txt_files = list(range(1, 61))\n",
        "\n",
        "    for file in txt_files:\n",
        "        print(f\"Processing image{file}:\")\n",
        "        process_txt(file)"
      ],
      "metadata": {
        "id": "NspQQlOSGQAm",
        "outputId": "67c6ca51-10cb-473e-cae0-c99a8d29f5e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing image1:\n",
            "Downloaded 0 files for image1.\n",
            "Error processing /content/sample_data/out/image-sitemap-1.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image1.zip'\n",
            "Processing image2:\n",
            "Downloaded 0 files for image2.\n",
            "Error processing /content/sample_data/out/image-sitemap-2.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image2.zip'\n",
            "Processing image3:\n",
            "Downloaded 0 files for image3.\n",
            "Error processing /content/sample_data/out/image-sitemap-3.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image3.zip'\n",
            "Processing image4:\n",
            "Downloaded 0 files for image4.\n",
            "Error processing /content/sample_data/out/image-sitemap-4.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image4.zip'\n",
            "Processing image5:\n",
            "Downloaded 0 files for image5.\n",
            "Error processing /content/sample_data/out/image-sitemap-5.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image5.zip'\n",
            "Processing image6:\n",
            "Downloaded 0 files for image6.\n",
            "Error processing /content/sample_data/out/image-sitemap-6.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image6.zip'\n",
            "Processing image7:\n",
            "Downloaded 0 files for image7.\n",
            "Error processing /content/sample_data/out/image-sitemap-7.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image7.zip'\n",
            "Processing image8:\n",
            "Downloaded 0 files for image8.\n",
            "Error processing /content/sample_data/out/image-sitemap-8.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image8.zip'\n",
            "Processing image9:\n",
            "Downloaded 0 files for image9.\n",
            "Error processing /content/sample_data/out/image-sitemap-9.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image9.zip'\n",
            "Processing image10:\n",
            "Downloaded 0 files for image10.\n",
            "Error processing /content/sample_data/out/image-sitemap-10.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image10.zip'\n",
            "Processing image11:\n",
            "Downloaded 0 files for image11.\n",
            "Error processing /content/sample_data/out/image-sitemap-11.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image11.zip'\n",
            "Processing image12:\n",
            "Downloaded 0 files for image12.\n",
            "Error processing /content/sample_data/out/image-sitemap-12.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image12.zip'\n",
            "Processing image13:\n",
            "Downloaded 0 files for image13.\n",
            "Error processing /content/sample_data/out/image-sitemap-13.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image13.zip'\n",
            "Processing image14:\n",
            "Downloaded 0 files for image14.\n",
            "Error processing /content/sample_data/out/image-sitemap-14.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image14.zip'\n",
            "Processing image15:\n",
            "Downloaded 0 files for image15.\n",
            "Error processing /content/sample_data/out/image-sitemap-15.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image15.zip'\n",
            "Processing image16:\n",
            "Downloaded 0 files for image16.\n",
            "Error processing /content/sample_data/out/image-sitemap-16.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image16.zip'\n",
            "Processing image17:\n",
            "Downloaded 0 files for image17.\n",
            "Error processing /content/sample_data/out/image-sitemap-17.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image17.zip'\n",
            "Processing image18:\n",
            "Downloaded 0 files for image18.\n",
            "Error processing /content/sample_data/out/image-sitemap-18.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image18.zip'\n",
            "Processing image19:\n",
            "Downloaded 0 files for image19.\n",
            "Error processing /content/sample_data/out/image-sitemap-19.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image19.zip'\n",
            "Processing image20:\n",
            "Downloaded 0 files for image20.\n",
            "Error processing /content/sample_data/out/image-sitemap-20.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image20.zip'\n",
            "Processing image21:\n",
            "Downloaded 0 files for image21.\n",
            "Error processing /content/sample_data/out/image-sitemap-21.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image21.zip'\n",
            "Processing image22:\n",
            "Downloaded 0 files for image22.\n",
            "Error processing /content/sample_data/out/image-sitemap-22.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image22.zip'\n",
            "Processing image23:\n",
            "Downloaded 0 files for image23.\n",
            "Error processing /content/sample_data/out/image-sitemap-23.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image23.zip'\n",
            "Processing image24:\n",
            "Downloaded 0 files for image24.\n",
            "Error processing /content/sample_data/out/image-sitemap-24.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image24.zip'\n",
            "Processing image25:\n",
            "Downloaded 0 files for image25.\n",
            "Error processing /content/sample_data/out/image-sitemap-25.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image25.zip'\n",
            "Processing image26:\n",
            "Downloaded 0 files for image26.\n",
            "Error processing /content/sample_data/out/image-sitemap-26.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image26.zip'\n",
            "Processing image27:\n",
            "Downloaded 0 files for image27.\n",
            "Error processing /content/sample_data/out/image-sitemap-27.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image27.zip'\n",
            "Processing image28:\n",
            "Downloaded 0 files for image28.\n",
            "Error processing /content/sample_data/out/image-sitemap-28.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image28.zip'\n",
            "Processing image29:\n",
            "Downloaded 0 files for image29.\n",
            "Error processing /content/sample_data/out/image-sitemap-29.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image29.zip'\n",
            "Processing image30:\n",
            "Downloaded 0 files for image30.\n",
            "Error processing /content/sample_data/out/image-sitemap-30.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image30.zip'\n",
            "Processing image31:\n",
            "Downloaded 0 files for image31.\n",
            "Error processing /content/sample_data/out/image-sitemap-31.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image31.zip'\n",
            "Processing image32:\n",
            "Downloaded 0 files for image32.\n",
            "Error processing /content/sample_data/out/image-sitemap-32.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image32.zip'\n",
            "Processing image33:\n",
            "Downloaded 0 files for image33.\n",
            "Error processing /content/sample_data/out/image-sitemap-33.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image33.zip'\n",
            "Processing image34:\n",
            "Downloaded 0 files for image34.\n",
            "Error processing /content/sample_data/out/image-sitemap-34.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image34.zip'\n",
            "Processing image35:\n",
            "Downloaded 0 files for image35.\n",
            "Error processing /content/sample_data/out/image-sitemap-35.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image35.zip'\n",
            "Processing image36:\n",
            "Downloaded 0 files for image36.\n",
            "Error processing /content/sample_data/out/image-sitemap-36.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image36.zip'\n",
            "Processing image37:\n",
            "Downloaded 0 files for image37.\n",
            "Error processing /content/sample_data/out/image-sitemap-37.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image37.zip'\n",
            "Processing image38:\n",
            "Downloaded 0 files for image38.\n",
            "Error processing /content/sample_data/out/image-sitemap-38.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image38.zip'\n",
            "Processing image39:\n",
            "Downloaded 0 files for image39.\n",
            "Error processing /content/sample_data/out/image-sitemap-39.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image39.zip'\n",
            "Processing image40:\n",
            "Downloaded 0 files for image40.\n",
            "Error processing /content/sample_data/out/image-sitemap-40.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image40.zip'\n",
            "Processing image41:\n",
            "Downloaded 0 files for image41.\n",
            "Error processing /content/sample_data/out/image-sitemap-41.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image41.zip'\n",
            "Processing image42:\n",
            "Downloaded 0 files for image42.\n",
            "Error processing /content/sample_data/out/image-sitemap-42.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image42.zip'\n",
            "Processing image43:\n",
            "Downloaded 0 files for image43.\n",
            "Error processing /content/sample_data/out/image-sitemap-43.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image43.zip'\n",
            "Processing image44:\n",
            "Downloaded 0 files for image44.\n",
            "Error processing /content/sample_data/out/image-sitemap-44.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image44.zip'\n",
            "Processing image45:\n",
            "Downloaded 0 files for image45.\n",
            "Error processing /content/sample_data/out/image-sitemap-45.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image45.zip'\n",
            "Processing image46:\n",
            "Downloaded 0 files for image46.\n",
            "Error processing /content/sample_data/out/image-sitemap-46.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image46.zip'\n",
            "Processing image47:\n",
            "Downloaded 0 files for image47.\n",
            "Error processing /content/sample_data/out/image-sitemap-47.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image47.zip'\n",
            "Processing image48:\n",
            "Downloaded 0 files for image48.\n",
            "Error processing /content/sample_data/out/image-sitemap-48.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image48.zip'\n",
            "Processing image49:\n",
            "Downloaded 0 files for image49.\n",
            "Error processing /content/sample_data/out/image-sitemap-49.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image49.zip'\n",
            "Processing image50:\n",
            "Downloaded 0 files for image50.\n",
            "Error processing /content/sample_data/out/image-sitemap-50.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image50.zip'\n",
            "Processing image51:\n",
            "Downloaded 0 files for image51.\n",
            "Error processing /content/sample_data/out/image-sitemap-51.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image51.zip'\n",
            "Processing image52:\n",
            "Downloaded 0 files for image52.\n",
            "Error processing /content/sample_data/out/image-sitemap-52.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image52.zip'\n",
            "Processing image53:\n",
            "Downloaded 0 files for image53.\n",
            "Error processing /content/sample_data/out/image-sitemap-53.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image53.zip'\n",
            "Processing image54:\n",
            "Downloaded 0 files for image54.\n",
            "Error processing /content/sample_data/out/image-sitemap-54.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image54.zip'\n",
            "Processing image55:\n",
            "Downloaded 0 files for image55.\n",
            "Error processing /content/sample_data/out/image-sitemap-55.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image55.zip'\n",
            "Processing image56:\n",
            "Downloaded 0 files for image56.\n",
            "Error processing /content/sample_data/out/image-sitemap-56.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image56.zip'\n",
            "Processing image57:\n",
            "Downloaded 0 files for image57.\n",
            "Error processing /content/sample_data/out/image-sitemap-57.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image57.zip'\n",
            "Processing image58:\n",
            "Downloaded 0 files for image58.\n",
            "Error processing /content/sample_data/out/image-sitemap-58.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image58.zip'\n",
            "Processing image59:\n",
            "Downloaded 0 files for image59.\n",
            "Error processing /content/sample_data/out/image-sitemap-59.txt: [Errno 2] No such file or directory: '/content/sample_data/imagebig/image59.zip'\n",
            "Processing image60:\n",
            "Error processing /content/sample_data/out/image-sitemap-60.txt: [Errno 2] No such file or directory: '/content/sample_data/out/image-sitemap-60.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "跳过以下步骤"
      ],
      "metadata": {
        "id": "DuzuFK6lUzVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " import os\n",
        "\n",
        " with open('/content/sample_data/outtxt/image-sitemap-1.txt') as f:\n",
        "     lines = f.readlines()\n",
        "\n",
        " out_lines = []\n",
        " for line in lines:\n",
        "     urls = line.split('>')[-2].split('<')[-1].split('|')\n",
        "     for url in urls:\n",
        "         if url.rfind('-') > 0 and url.rfind('-') < len(url)-1:\n",
        "             if url[url.rfind('-')+1].isdigit() and url[url.rfind('-')+2].isalpha():\n",
        "                 out_lines.append(line)\n",
        "                 break\n",
        "\n",
        " with open(os.path.join('/content/sample_data/out', 'image-sitemap-1.txt'), 'w') as f:\n",
        "     for line in out_lines:\n",
        "         f.write(line)"
      ],
      "metadata": {
        "id": "jKITgtiUGlZf",
        "outputId": "0452487c-8bab-4e5e-87ec-7bc334c4f11c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-b0a3f6325012>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mout_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " import os\n",
        "\n",
        " input_file = '/content/sample_data/outtxt/image-sitemap-1.txt'\n",
        " output_file = '/content/sample_data/out/image-sitemap-1.txt'\n",
        "\n",
        " with open(input_file, 'r') as f:\n",
        "     lines = f.readlines()\n",
        "\n",
        " result = []\n",
        " for line in lines:\n",
        "     if '-e' in line:\n",
        "         result.append(line)\n",
        "\n",
        " with open(output_file, 'w') as f:\n",
        "     for line in result:\n",
        "         f.write(line)"
      ],
      "metadata": {
        "id": "gRUGB_1-LYce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " import os\n",
        "\n",
        " input_file = '/content/sample_data/outtxt/image-sitemap-1.txt'\n",
        " output_file = '/content/sample_data/out/image-sitemap-1.txt'\n",
        "\n",
        " with open(input_file, 'r') as f:\n",
        "     lines = f.readlines()\n",
        "\n",
        " output = []\n",
        " for line in lines:\n",
        "     if '-e' in line:\n",
        "         output.append(line)\n",
        "\n",
        " with open(output_file, 'w') as f:\n",
        "     f.writelines(output)"
      ],
      "metadata": {
        "id": "wBByLvYgMA-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        " import os\n",
        "\n",
        " input_file = '/content/sample_data/outtxt/image-sitemap-1.txt'\n",
        " output_file = '/content/sample_data/out/image-sitemap-1.txt'\n",
        "\n",
        " with open(input_file, 'r') as f:\n",
        "     lines = f.readlines()\n",
        "\n",
        " output = []\n",
        " for line in lines:\n",
        "     link = line[line.find('https'):]\n",
        "     if 'http' in link:\n",
        "         output.append(link)\n",
        "\n",
        " with open(output_file, 'w') as f:\n",
        "     f.writelines(output)"
      ],
      "metadata": {
        "id": "hJ13KwuUMvbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " import os\n",
        "\n",
        " input_file = '/content/sample_data/outtxt/image-sitemap-1.txt'\n",
        " output_file = '/content/sample_data/out/image-sitemap-1.txt'\n",
        "\n",
        " with open(input_file, 'r') as f:\n",
        "     lines = f.readlines()\n",
        "\n",
        " output = []\n",
        " for line in lines:\n",
        "     link = line[line.find('https'):]\n",
        "     if '-e' in link and link[link.find('-e')+2].isdigit():\n",
        "         output.append(link)\n",
        "\n",
        " with open(output_file, 'w') as f:\n",
        "     f.writelines(output)"
      ],
      "metadata": {
        "id": "fycwDrccNbbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        " import os\n",
        "\n",
        " input_file = '/content/sample_data/outtxt/image-sitemap-1.txt'\n",
        " output_file = '/content/sample_data/out/image-sitemap-1.txt'\n",
        "\n",
        " with open(input_file, 'r') as f:\n",
        "     lines = f.readlines()\n",
        "\n",
        " output = []\n",
        " for line in lines:\n",
        "     link = line[line.find('https'):]\n",
        "     if '-e' in link and link[link.find('-e')+2].isdigit():\n",
        "         output.append(line)\n",
        "     else:\n",
        "         # 删除该行\n",
        "         line = ''\n",
        "\n",
        " with open(output_file, 'w') as f:\n",
        "     f.writelines(output)"
      ],
      "metadata": {
        "id": "4gJl0n37OcAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " import os\n",
        "\n",
        " input_file = '/content/sample_data/outtxt/image-sitemap-1.txt'\n",
        " output_file = '/content/sample_data/out/image-sitemap-1.txt'\n",
        "\n",
        " count = 0\n",
        " with open(input_file, 'r') as f:\n",
        "     lines = f.readlines()\n",
        "\n",
        " output = []\n",
        " for line in lines:\n",
        "     link = line[line.find('https'):]\n",
        "     if '-e' in link and link[link.find('-e')+2].isdigit():\n",
        "         output.append(line)\n",
        "         count += 1      # 增加计数\n",
        "     else:\n",
        "         line = ''\n",
        "\n",
        " print(f'共输出了{count}个符合条件的链接')\n",
        "\n",
        " with open(output_file, 'w') as f:\n",
        "     f.writelines(output)"
      ],
      "metadata": {
        "id": "PydWBAiUO1cC",
        "outputId": "1e689394-c8b9-4323-f64a-f0668be42fff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "共输出了1个符合条件的链接\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " with open('/content/sample_data/outtxt/image-sitemap-1.txt') as f:\n",
        "      links = f.read()\n",
        "\n",
        "      link_list = links.split('https://')\n",
        "\n",
        "      count = 0\n",
        " for link in link_list:\n",
        "      if '-e' in link and link[-2] in '123456789':\n",
        "          count += 1\n",
        "          print(f'https://{link}')\n",
        "\n",
        " print(f'Total links with -e and number: {count}')"
      ],
      "metadata": {
        "id": "KLkkokeYPjZc",
        "outputId": "51ced563-d1f5-4e82-c3b7-3a422b9c3f18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total links with -e and number: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/sample_data/outtxt/image-sitemap-1.txt') as f:\n",
        "     links = f.read()\n",
        "\n",
        " link_list = links.split('https://')  # Split into links\n",
        "\n",
        " count = 0\n",
        "for link in link_list:\n",
        "     if '-e' in link:\n",
        "         count += 1\n",
        "         print(f'https://{link}')\n",
        "\n",
        "print(f'Total links with -e: {count}')"
      ],
      "metadata": {
        "id": "LF7OHJNKPzA0",
        "outputId": "6ed21d35-ed31-40e5-a717-095186d1ec75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    link_list = links.split('https://')  # Split into links\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/sample_data/outtxt/image-sitemap-1.txt') as f:\n",
        "     links = f.read()\n",
        "\n",
        " link_list = links.split('https://')  # Split into links\n",
        "\n",
        " count = 0\n",
        "for link in link_list:\n",
        "     if '-e' in link:\n",
        "         count += 1\n",
        "         print(f'https://{link}')\n",
        "\n",
        "print(f'Total links with -e: {count}')"
      ],
      "metadata": {
        "id": "jBnyP1ZWQobu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/sample_data/outtxt/image-sitemap-1.txt') as f:\n",
        "     links = f.read()\n",
        "\n",
        "     link_list = links.split('https://')  # Split into links\n",
        "\n",
        "     count = 0\n",
        "for link in link_list:\n",
        "     if '-e' in link:\n",
        "         count += 1\n",
        "         print(f'https://{link}')\n",
        "\n",
        "print(f'Total links with -e: {count}')"
      ],
      "metadata": {
        "id": "M0aqljYcRCS4",
        "outputId": "7d84b0bb-8de7-48cb-957d-495b56266de7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://aniverse-mag.com/wp-content/uploads/2017/09/2f70466fc3e77d55f0ca29b565144e70-e1505384473333.png\n",
            "https://aniverse-mag.com/wp-content/uploads/2016/12/41fba42bd17980e5803def4aa004fbc9-e1505383315294.png\n",
            "https://aniverse-mag.com/wp-content/uploads/2016/12/41fba42bd17980e5803def4aa004fbc9-1-e1505385224891.png\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/CAPCOM-2-e1507004528277.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/DMM-5-e1507005814205.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/Flyhigh-Works-3-e1507007160246.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/Flyhigh-Works-4-e1507007233700.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/MONSTER-2-e1507011023568.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/SEGA-6-e1507010668354.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/SQUARE-ENIX-4-e1507010799672.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/418f539db88e23c78aa774f09595730b-e1507010926894.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/06d3ae6dc73306cbee9458e11fe22a19-e1507011108347.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/b30d507db7afb5b7927e393ae18538b2-e1507089157712.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/SB-ep02_0023_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/SB-ep02_0023_result-1.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/SB-ep02_0069_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/SB-ep02_0143_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/SB-ep02_0149_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/SB-ep02_0193_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/SB-ep02_0197_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/SB-ep02_0214_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/SB-ep02_0247_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/designevent_main-e1508383144102.png\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/0c0797495dbbaf99edf36896d15122d0-1-e1508907601709.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/c25ca571b38c2857e3c90e175481d3e9-e1509009842394.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/kudo_morikubo-e1509012243208.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/SB-ep03_0047_result-e1509070854875.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/SB-ep03_0060_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/SB-ep03_0071_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/SB-ep03_0088_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/SB-ep03_0110_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/SB-ep03_0129_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/SB-ep03_0150_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/SB-ep03_0168_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/image2-e1509100083346.png\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/L4A5612_small-e1509330344151.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/fa029687d480316bf64d7a6426d859b5-e1509357902712.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/10/f95c0c00eda1f1bacaef33bedeadce6c-e1509440815811.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/f95c0c00eda1f1bacaef33bedeadce6c-e1509440952771.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep04_0008_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep04_0020_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep04_0037_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep04_0062_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep04_0078_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep04_0098_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep04_0113_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep04_0154_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/3_st1-1-e1509522794265.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/0c0797495dbbaf99edf36896d15122d0-e1509529312380.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/c0526f8aa3ff097f95ad9aecf9afa1fd-1-e1509529855803.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/image3-e1509592259599.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/7a2ff8e00e0382bce1e4194f91343d7b-e1509952233412.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/ec669961315c0dea7bc4db015031fef8-1-e1509960904561.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/c94f3348d634843da7b492cba361c164-e1509963185262.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/chara-1-e1510027511231.png\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/1.Pslive_Logo_Gaitame--e1510040242575.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/2.mimori-e1510039461647.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/3.taketatsu-e1510035258339.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/4.uchida-e1510039501398.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/5.MICHI_-e1510039738661.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/6.OxT_-e1510039675290.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/fe48ab5f42b7f13ad78c578032ac3b98-e1510048648523.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/genga-e1510110061268.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/image1-e1510114065785.jpeg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/konosekai-1-e1510125415775.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/m-67-1-e1510132530738.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep05_0000_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep05_0062_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep05_0063_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep05_0170_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/BR1_jk-1-1-e1510570285951.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep06_0007_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep06_0064_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep06_0088_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep06_0093_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep06_0116_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep06_0129_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep06_0139_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep06_0178_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep07tif_0001_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep07tif_0037_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep07tif_0046_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep07tif_0048_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep07tif_0085_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep07tif_0088_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep07tif_0095_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/SB-ep07tif_0097_result.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2017/11/7092a946899ee7be474127156cda4113-e1512730287166.png\n",
            "https://aniverse-mag.com/wp-content/uploads/2018/04/3fff4c85a5f0118539237b64bb59cce0-e1523003994193.jpg\n",
            "https://aniverse-mag.com/wp-content/uploads/2018/04/e9b2c3cbd62843064f0e7a3d9abd7e73-e1523003964874.jpg\n",
            "Total links with -e: 90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " with open('/content/sample_data/outtxt/image-sitemap-1.txt') as f:\n",
        "      links = f.read()\n",
        "\n",
        "      link_list = links.split('https://')  # Split into links\n",
        "\n",
        "      count = 0\n",
        "      with open('/content/sample_data/out/image-sitemap-1.txt', 'w') as outf:\n",
        "       for link in link_list:\n",
        "        if '-e' in link:\n",
        "          count += 1\n",
        "          outf.write(f'https://{link}\\n')"
      ],
      "metadata": {
        "id": "GlyHGm0OSmGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "以上都不用执行"
      ],
      "metadata": {
        "id": "5XyvewFaUujm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " for x in range(1, 60):\n",
        "     with open(f'/content/sample_data/outtxt/image-sitemap-{x}.txt') as f:\n",
        "         links = f.read()\n",
        "\n",
        "         link_list = links.split('https://')\n",
        "\n",
        "         count = 0\n",
        "         with open(f'/content/sample_data/out/image-sitemap-{x}.txt', 'w') as outf:\n",
        "             for link in link_list:\n",
        "                 if '-e' in link:\n",
        "                     link = link[:link.index('-e')]\n",
        "                     count += 1\n",
        "                     outf.write(f'https://{link}\\n')"
      ],
      "metadata": {
        "id": "ir495gvCddZu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " for x in range(1, 60):\n",
        "     with open(f'/content/sample_data/outtxt/image-sitemap-{x}.txt') as f:\n",
        "         links = f.read()\n",
        "\n",
        "         link_list = links.split('https://')\n",
        "\n",
        "         count = 0\n",
        "         with open(f'/content/sample_data/out/image-sitemap-{x}.txt', 'w') as outf:\n",
        "             for link in link_list:\n",
        "                 if '-e Parameters' in link:\n",
        "                     link = link[:link.index('-e Parameters')]\n",
        "                     count += 1\n",
        "                     outf.write(f'https://{link}\\n')\n",
        "                 else:\n",
        "                     outf.write(f'https://{link}\\n')"
      ],
      "metadata": {
        "id": "pyunlLfwd61B"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " for x in range(1, 60):\n",
        "      with open(f'/content/sample_data/outtxt/image-sitemap-{x}.txt') as f:\n",
        "          links = f.read()\n",
        "\n",
        "          link_list = links.split('https://')\n",
        "\n",
        "          count = 0\n",
        "          with open(f'/content/sample_data/out/image-sitemap-{x}.txt', 'w') as outf:\n",
        "              for link in link_list:\n",
        "                  if '-e' in link:\n",
        "                      index = link.index('-e')\n",
        "                      link = link[:index] + link[link.index('.', index):]\n",
        "                      count += 1\n",
        "                      outf.write(f'https://{link}\\n')"
      ],
      "metadata": {
        "id": "mxF09xoTeg8w"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新段落"
      ],
      "metadata": {
        "id": "OJAPlv-lekb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " for x in range(1, 60):\n",
        "     with open(f'/content/sample_data/outtxt/image-sitemap-{x}.txt') as f:\n",
        "         links = f.read()\n",
        "\n",
        "         link_list = links.split('https://')\n",
        "\n",
        "         count = 0\n",
        "         with open(f'/content/sample_data/out/image-sitemap-{x}.txt', 'w') as outf:\n",
        "             for link in link_list:\n",
        "                 if '-e' in link:\n",
        "                     count += 1\n",
        "                     outf.write(f'https://{link}\\n')"
      ],
      "metadata": {
        "id": "R50mv_bsUYXB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " import requests\n",
        " import os\n",
        " import zipfile\n",
        " import shutil\n",
        " from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        " out_dir = '/content/sample_data/out'\n",
        " img_dir = '/content/sample_data/out/imageof'\n",
        "\n",
        " os.makedirs(img_dir, exist_ok=True)\n",
        "\n",
        " def download_image(url):\n",
        "     img_name = url.split('/')[-1]\n",
        "     r = requests.get(url, allow_redirects=True)\n",
        "     with open(os.path.join(img_dir, img_name), 'wb') as f:\n",
        "         f.write(r.content)\n",
        "\n",
        " with ThreadPoolExecutor(max_workers=64) as executor:\n",
        "     for x in range(1, 61):\n",
        "         with open(f'{out_dir}/image-sitemap-{x}.txt') as f:\n",
        "             link_list = f.read().split()\n",
        "             img_urls = [url for url in link_list if url.startswith('https://')]\n",
        "\n",
        "         futures = [executor.submit(download_image, url) for url in img_urls]\n",
        "         for future in as_completed(futures):\n",
        "             future.result()"
      ],
      "metadata": {
        "id": "q5QEXajMXP8p",
        "outputId": "c7284372-af2e-4399-fb26-64531d34524b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-bfe0983d15b4>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m61\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{out_dir}/image-sitemap-{x}.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mlink_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mimg_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlink_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/sample_data/out/image-sitemap-60.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " import os\n",
        " import zipfile\n",
        "\n",
        " out_dir = '/content/sample_data/out'\n",
        " img_dir = '/content/sample_data/out/imageof'\n",
        " zip_file = '/content/sample_data/imageof.zip'\n",
        "\n",
        " # Zip the imageof directory\n",
        " with zipfile.ZipFile(zip_file, 'w') as zip_f:\n",
        "     for root, dirs, files in os.walk(img_dir):\n",
        "         for file in files:\n",
        "             zip_f.write(os.path.join(root, file))\n",
        "\n",
        " # Get zip file size\n",
        " stat_info = os.stat(zip_file)\n",
        " size_mb = stat_info.st_size / (1024 * 1024)\n",
        " print(f'Zip file size: {size_mb:.2f} MB')"
      ],
      "metadata": {
        "id": "pdShpIX2TMCL",
        "outputId": "5d7251e8-a5f8-4413-eda4-abed9d9781d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zip file size: 18539.19 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " import os\n",
        " import zipfile\n",
        "\n",
        " out_dir = '/content/sample_data/out'\n",
        " img_dir = '/content/sample_data/out/imageof'\n",
        " zip_file = '/content/sample_data/imageof.zip'\n",
        "\n",
        " # Get total size of imageof folder\n",
        " total_size = 0\n",
        " for root, dirs, files in os.walk(img_dir):\n",
        "     for file in files:\n",
        "         total_size += os.path.getsize(os.path.join(root, file))\n",
        "\n",
        " # Split into 5 equal parts\n",
        " part_size = total_size // 5\n",
        "\n",
        " # Zip the imageof directory in parts\n",
        " for i in range(5):\n",
        "     zip_file_part = zip_file.replace('.zip', f'_part{i}.zip')\n",
        "     with zipfile.ZipFile(zip_file_part, 'w') as zip_f:\n",
        "         size_so_far = 0\n",
        "         for root, dirs, files in os.walk(img_dir):\n",
        "             for file in files:\n",
        "                 filename = os.path.join(root, file)\n",
        "                 file_size = os.path.getsize(filename)\n",
        "                 if size_so_far + file_size < part_size:\n",
        "                     zip_f.write(filename)\n",
        "                     size_so_far += file_size\n",
        "                 else:\n",
        "                     break\n",
        "\n",
        "     # Get zip file part size\n",
        "     stat_info = os.stat(zip_file_part)\n",
        "     size_mb = stat_info.st_size / (1024 * 1024)\n",
        "     print(f'Part {i} size: {size_mb:.2f} MB')"
      ],
      "metadata": {
        "id": "NQKhU3o2kdjL",
        "outputId": "265367b6-e13d-4da3-c556-d76da857cad5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part 0 size: 3706.67 MB\n",
            "Part 1 size: 3706.67 MB\n",
            "Part 2 size: 3706.67 MB\n",
            "Part 3 size: 3706.67 MB\n",
            "Part 4 size: 3706.67 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "img_dir = '/content/sample_data/out/imageof'\n",
        "\n",
        "stat_info = os.stat(img_dir)\n",
        "size_mb = stat_info.st_size / (1024 * 1024)\n",
        "print(f'img file size: {size_mb:.2f} MB')"
      ],
      "metadata": {
        "id": "9kvWQNrxg5L3",
        "outputId": "f051d1fb-7219-4e63-f43d-fbddcdf2ca79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img file size: 0.49 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " import os\n",
        " import shutil\n",
        "\n",
        " out_dir = '/content/sample_data'\n",
        " drive_dir = '/content/drive/MyDrive'\n",
        " zip_file = '/content/sample_data/imageof_part0.zip'\n",
        "\n",
        " # Move zip file to drive directory\n",
        " shutil.move(zip_file, drive_dir)"
      ],
      "metadata": {
        "id": "CBTrEhtgY9pj",
        "outputId": "6801f0bc-e9b9-4a91-fff0-46683610b5bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/imageof_part0.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}